{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ce74f54-32fd-476b-9988-eb422d80d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from models import DiT_models\n",
    "from download import find_model\n",
    "from diffusion import create_diffusion\n",
    "from diffusers.models import AutoencoderKL\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "import argparse\n",
    "from torchvision.utils import save_image\n",
    "from timm.models.vision_transformer import Attention, Mlp\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b726e6bb-fcae-40a8-b5c7-913eaa6ca00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 24 21:18:13 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  |   00000000:CA:00.0 Off |                   On |\n",
      "| N/A   25C    P0             48W /  400W |   19989MiB /  40960MiB |     N/A      Default |\n",
      "|                                         |                        |              Enabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                            |\n",
      "+------------------+----------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                     Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                       BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                  |        ECC|                       |\n",
      "|==================+==================================+===========+=======================|\n",
      "|  0    2   0   0  |           19952MiB / 19968MiB    | 42      0 |  3   0    2    0    0 |\n",
      "|                  |                 2MiB / 32767MiB  |           |                       |\n",
      "+------------------+----------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0    2    0    2626856      C   ...gzeyuan/.conda/envs/qdit/bin/python      19906MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14099eb-e7c3-4c7a-ba1b-5740709faa57",
   "metadata": {},
   "source": [
    "### Load functions for quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb2a5653-8be1-43d1-9c93-1cfd24a90d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_channel_absmax(w, n_bits=8):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_tensor_absmax(w, n_bits=8):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_token_absmax(t, n_bits=8):\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_tensor_absmax(t, n_bits=8):\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "\n",
    "class W8A8Linear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        act_quant=\"per_token\",\n",
    "        quantize_output=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.randn(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.float16,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.zeros(\n",
    "                    (1, self.out_features), dtype=torch.float16, requires_grad=False\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "        if act_quant == \"per_token\":\n",
    "            self.act_quant_name = \"per_token\"\n",
    "            self.act_quant = partial(quantize_activation_per_token_absmax, n_bits=8)\n",
    "        elif act_quant == \"per_tensor\":\n",
    "            self.act_quant_name = \"per_tensor\"\n",
    "            self.act_quant = partial(quantize_activation_per_tensor_absmax, n_bits=8)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid act_quant: {act_quant}\")\n",
    "\n",
    "        if quantize_output:\n",
    "            self.output_quant_name = self.act_quant_name\n",
    "            self.output_quant = self.act_quant\n",
    "        else:\n",
    "            self.output_quant_name = \"None\"\n",
    "            self.output_quant = lambda x: x\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super(W8A8Linear, self).to(*args, **kwargs)\n",
    "        self.weight = self.weight.to(*args, **kwargs)\n",
    "        if self.bias is not None:\n",
    "            self.bias = self.bias.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        q_x = self.act_quant(x)\n",
    "        y = torch.functional.F.linear(q_x, self.weight, self.bias)\n",
    "        q_y = self.output_quant(y)\n",
    "        return q_y\n",
    "\n",
    "    @staticmethod\n",
    "    def from_float(\n",
    "        module, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_output=False\n",
    "    ):\n",
    "        assert isinstance(module, torch.nn.Linear)\n",
    "        new_module = W8A8Linear(\n",
    "            module.in_features,\n",
    "            module.out_features,\n",
    "            module.bias is not None,\n",
    "            act_quant=act_quant,\n",
    "            quantize_output=quantize_output,\n",
    "        )\n",
    "        if weight_quant == \"per_channel\":\n",
    "            new_module.weight = quantize_weight_per_channel_absmax(\n",
    "                module.weight, n_bits=4\n",
    "            )  # use 8-bit integer for weight\n",
    "        elif weight_quant == \"per_tensor\":\n",
    "            new_module.weight = quantize_weight_per_tensor_absmax(\n",
    "                module.weight, n_bits=4\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_quant: {weight_quant}\")\n",
    "        new_module.weight_quant_name = weight_quant\n",
    "        if module.bias is not None:\n",
    "            new_module.bias = module.bias\n",
    "        return new_module\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"W8A8Linear({self.in_features}, {self.out_features}, bias={self.bias is not None}, weight_quant={self.weight_quant_name}, act_quant={self.act_quant_name}, output_quant={self.output_quant_name})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06480189-cc6b-4ac3-84e7-5ffb1b56e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_dit_all_linear_layer(\n",
    "    model, weight_quant=\"per_tensor\", act_quant=\"per_tensor\", quantize_bmm_input=False\n",
    "):\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, torch.nn.Linear):  # quantize each linear layer\n",
    "            m = W8A8Linear.from_float(\n",
    "                m, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input\n",
    "            )\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0a0e03-09bc-4331-9477-e07cc2d16cd8",
   "metadata": {},
   "source": [
    "### Load functions for LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d795172c-8cd0-4550-bf54-366ea4958705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 34745\n",
       "    Root location: /n/home11/mingzeyuan/.cache/kagglehub/datasets/ifigotin/imagenetmini-1000/versions/1/imagenet-mini/train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
       "           )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "image_size = 256\n",
    "# Dataset and Dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "# Specify the target classes\n",
    "selected_class_ids = [0, 1, 2, 3]\n",
    "\n",
    "full_dataset = ImageFolder(\"/n/home11/mingzeyuan/.cache/kagglehub/datasets/ifigotin/imagenetmini-1000/versions/1/imagenet-mini/train\", transform=transform)\n",
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9402fd43-babc-4a13-8ff7-96a0b41a714b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1008a04349f4dd290ac168ef98e8a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "selected_indices = []\n",
    "for i, (_, label) in tqdm(enumerate(full_dataset)):\n",
    "    if label in selected_class_ids:\n",
    "        selected_indices.append(i)\n",
    "    if label > max(selected_class_ids):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69a1fdb1-3788-4db0-8b1a-778ab546e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = Subset(full_dataset, selected_indices)\n",
    "dataloader = DataLoader(filtered_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e88bef9-1ba1-468b-86a5-a083b19d5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define LoRA Layer\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, original_layer, r=8, alpha=1.0):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.r = r  # Low rank\n",
    "        self.alpha = alpha  # Scaling factor\n",
    "\n",
    "        # Low-rank matrices\n",
    "        self.A = nn.Parameter(torch.randn(original_layer.out_features, r) * 0.01)\n",
    "        self.B = nn.Parameter(torch.randn(r, original_layer.in_features) * 0.01)\n",
    "\n",
    "        # Scaling factor to ensure initial LoRA impact is small\n",
    "        self.scale = alpha / self.r\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora_adjustment = (x @ self.B.T) @ self.A.T  # (batch_size, in_features) -> (batch_size, out_features)\n",
    "        return self.original_layer(x) + lora_adjustment * self.scale\n",
    "\n",
    "def add_lora_to_model(model, r=8, alpha=1.0):\n",
    "    layers_to_modify = []  # Collect layers to modify first\n",
    "\n",
    "    # Collect all linear layers in a list\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and (\"mlp\" in name or \"attn\" in name):\n",
    "            layers_to_modify.append((name, module))\n",
    "\n",
    "    # Replace each collected layer with a LoRA layer\n",
    "    for name, module in layers_to_modify:\n",
    "        # Split the name by '.' to traverse submodules and set the new layer correctly\n",
    "        submodule = model\n",
    "        *module_names, layer_name = name.split(\".\")\n",
    "        for module_name in module_names:\n",
    "            submodule = getattr(submodule, module_name)\n",
    "\n",
    "        # Replace the layer with a LoRA layer\n",
    "        setattr(submodule, layer_name, LoRALayer(module, r=r, alpha=alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18a3b729-80e5-4c89-b380-cebe5b45016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `model` is the DiT model with LoRA layers added\n",
    "def freeze_model_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if not ((\".A\" in name) or ('.B' in name)):  # Replace with the identifier for LoRA parameters\n",
    "            param.requires_grad = False  # Freeze base model weights\n",
    "\n",
    "# Add LoRA layers (as shown in previous responses) and freeze base model weights\n",
    "# add_lora_to_model(model)  # Add LoRA layers to the model\n",
    "# freeze_model_weights(model)  # Freeze original model weights\n",
    "# model.to(\"cuda\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2221f2d-a0a0-44d9-9042-66d514c726f9",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff555994-13f4-4f78-8295-27c480b6ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256 #@param [256, 512]\n",
    "vae_model = \"stabilityai/sd-vae-ft-ema\" #@param [\"stabilityai/sd-vae-ft-mse\", \"stabilityai/sd-vae-ft-ema\"]\n",
    "# Load model:\n",
    "latent_size = int(image_size) // 8\n",
    "\n",
    "def init_models():\n",
    "    model = DiT_models['DiT-XL/2'](input_size=latent_size).to(device)\n",
    "    # state_dict = find_model(f\"DiT-XL-2-{image_size}x{image_size}.pt\")\n",
    "    state_dict = torch.load('/n/netscratch/nali_lab_seas/Everyone/mingze/models/pretrained_models/DiT-XL-2-256x256.pt', weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval() # important!\n",
    "    vae = AutoencoderKL.from_pretrained(vae_model).to(device)\n",
    "\n",
    "    return model, vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a0d09-d9a2-4428-b612-cee8a6f2f57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8fb265c-3fba-44e8-b320-a4fe799c61af",
   "metadata": {},
   "source": [
    "### FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f10f5dfa-a4d9-49e0-8c4d-4a24edd23864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 1 #@param {type:\"number\"}\n",
    "# torch.manual_seed(seed)\n",
    "# num_sampling_steps = 100 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "# cfg_scale = 2 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "# class_labels = selected_class_ids #@param {type:\"raw\"}\n",
    "# samples_per_row = 4 #@param {type:\"number\"}\n",
    "\n",
    "# model, vae = init_models()\n",
    "\n",
    "# # Create diffusion object:\n",
    "# diffusion = create_diffusion(str(num_sampling_steps))\n",
    "\n",
    "# # Create sampling noise:\n",
    "# n = len(class_labels)\n",
    "# z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
    "# y = torch.tensor(class_labels, device=device)\n",
    "\n",
    "# # Setup classifier-free guidance:\n",
    "# z = torch.cat([z, z], 0)\n",
    "# y_null = torch.tensor([1000] * n, device=device)\n",
    "# y = torch.cat([y, y_null], 0)\n",
    "# model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "\n",
    "# # Sample images:\n",
    "# samples = diffusion.p_sample_loop(\n",
    "#     model.forward_with_cfg, z.shape, z, clip_denoised=False,\n",
    "#     model_kwargs=model_kwargs, progress=True, device=device\n",
    "# )\n",
    "# samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "# samples = vae.decode(samples / 0.18215).sample\n",
    "\n",
    "# # Save and display images:\n",
    "# save_image(samples, \"sample.png\", nrow=int(samples_per_row),\n",
    "#            normalize=True, value_range=(-1, 1))\n",
    "# samples = Image.open(\"sample.png\")\n",
    "# display(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c7d967-4065-4f44-bcb3-71aafa722dd3",
   "metadata": {},
   "source": [
    "### W4A8 quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ef69836-2d2f-4a71-a44a-5f5af83f9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 1 #@param {type:\"number\"}\n",
    "# torch.manual_seed(seed)\n",
    "# num_sampling_steps = 100 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "# cfg_scale = 2 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "# class_labels = selected_class_ids #@param {type:\"raw\"}\n",
    "# samples_per_row = 4 #@param {type:\"number\"}\n",
    "\n",
    "# model, vae = init_models()\n",
    "\n",
    "# model = quantize_dit_all_linear_layer(model)\n",
    "\n",
    "# # Create diffusion object:\n",
    "# diffusion = create_diffusion(str(num_sampling_steps))\n",
    "\n",
    "# # Create sampling noise:\n",
    "# n = len(class_labels)\n",
    "# z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
    "# y = torch.tensor(class_labels, device=device)\n",
    "\n",
    "# # Setup classifier-free guidance:\n",
    "# z = torch.cat([z, z], 0)\n",
    "# y_null = torch.tensor([1000] * n, device=device)\n",
    "# y = torch.cat([y, y_null], 0)\n",
    "# model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "\n",
    "# # Sample images:\n",
    "# samples = diffusion.p_sample_loop(\n",
    "#     model.forward_with_cfg, z.shape, z, clip_denoised=False,\n",
    "#     model_kwargs=model_kwargs, progress=True, device=device\n",
    "# )\n",
    "# samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "# samples = vae.decode(samples / 0.18215).sample\n",
    "\n",
    "# # Save and display images:\n",
    "# save_image(samples, \"sample.png\", nrow=int(samples_per_row),\n",
    "#            normalize=True, value_range=(-1, 1))\n",
    "# samples = Image.open(\"sample.png\")\n",
    "# display(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac4bdde6-2422-4ce3-ba25-a2a90995c8cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  if param.requires_grad:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c549a8-668a-4eee-93eb-fc43449c7a05",
   "metadata": {},
   "source": [
    "### W4A8 quantization + LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6078a6be-e300-4b12-9569-59ceab8b89b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiT(\n",
       "  (x_embedder): PatchEmbed(\n",
       "    (proj): Conv2d(4, 1152, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (t_embedder): TimestepEmbedder(\n",
       "    (mlp): Sequential(\n",
       "      (0): LoRALayer(\n",
       "        (original_layer): Linear(in_features=256, out_features=1152, bias=True)\n",
       "      )\n",
       "      (1): SiLU()\n",
       "      (2): LoRALayer(\n",
       "        (original_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (y_embedder): LabelEmbedder(\n",
       "    (embedding_table): Embedding(1001, 1152)\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-27): 28 x DiTBlock(\n",
       "      (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn): Attention(\n",
       "        (qkv): LoRALayer(\n",
       "          (original_layer): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "        )\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): LoRALayer(\n",
       "          (original_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): LoRALayer(\n",
       "          (original_layer): Linear(in_features=1152, out_features=4608, bias=True)\n",
       "        )\n",
       "        (act): GELU(approximate='tanh')\n",
       "        (drop1): Dropout(p=0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): LoRALayer(\n",
       "          (original_layer): Linear(in_features=4608, out_features=1152, bias=True)\n",
       "        )\n",
       "        (drop2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (adaLN_modulation): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=1152, out_features=6912, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer): FinalLayer(\n",
       "    (norm_final): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)\n",
       "    (linear): Linear(in_features=1152, out_features=32, bias=True)\n",
       "    (adaLN_modulation): Sequential(\n",
       "      (0): SiLU()\n",
       "      (1): Linear(in_features=1152, out_features=2304, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1 #@param {type:\"number\"}\n",
    "torch.manual_seed(seed)\n",
    "num_sampling_steps = 100 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "cfg_scale = 2 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "class_labels = selected_class_ids #@param {type:\"raw\"}\n",
    "samples_per_row = 4 #@param {type:\"number\"}\n",
    "\n",
    "model, vae = init_models()\n",
    "\n",
    "model = quantize_dit_all_linear_layer(model)\n",
    "add_lora_to_model(model)  # Add LoRA layers to the model\n",
    "freeze_model_weights(model)  # Freeze original model weights\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25d6dee0-109e-4997-990e-8433090f2a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_embedder.mlp.0.A\n",
      "t_embedder.mlp.0.B\n",
      "t_embedder.mlp.2.A\n",
      "t_embedder.mlp.2.B\n",
      "blocks.0.attn.qkv.A\n",
      "blocks.0.attn.qkv.B\n",
      "blocks.0.attn.proj.A\n",
      "blocks.0.attn.proj.B\n",
      "blocks.0.mlp.fc1.A\n",
      "blocks.0.mlp.fc1.B\n",
      "blocks.0.mlp.fc2.A\n",
      "blocks.0.mlp.fc2.B\n",
      "blocks.1.attn.qkv.A\n",
      "blocks.1.attn.qkv.B\n",
      "blocks.1.attn.proj.A\n",
      "blocks.1.attn.proj.B\n",
      "blocks.1.mlp.fc1.A\n",
      "blocks.1.mlp.fc1.B\n",
      "blocks.1.mlp.fc2.A\n",
      "blocks.1.mlp.fc2.B\n",
      "blocks.2.attn.qkv.A\n",
      "blocks.2.attn.qkv.B\n",
      "blocks.2.attn.proj.A\n",
      "blocks.2.attn.proj.B\n",
      "blocks.2.mlp.fc1.A\n",
      "blocks.2.mlp.fc1.B\n",
      "blocks.2.mlp.fc2.A\n",
      "blocks.2.mlp.fc2.B\n",
      "blocks.3.attn.qkv.A\n",
      "blocks.3.attn.qkv.B\n",
      "blocks.3.attn.proj.A\n",
      "blocks.3.attn.proj.B\n",
      "blocks.3.mlp.fc1.A\n",
      "blocks.3.mlp.fc1.B\n",
      "blocks.3.mlp.fc2.A\n",
      "blocks.3.mlp.fc2.B\n",
      "blocks.4.attn.qkv.A\n",
      "blocks.4.attn.qkv.B\n",
      "blocks.4.attn.proj.A\n",
      "blocks.4.attn.proj.B\n",
      "blocks.4.mlp.fc1.A\n",
      "blocks.4.mlp.fc1.B\n",
      "blocks.4.mlp.fc2.A\n",
      "blocks.4.mlp.fc2.B\n",
      "blocks.5.attn.qkv.A\n",
      "blocks.5.attn.qkv.B\n",
      "blocks.5.attn.proj.A\n",
      "blocks.5.attn.proj.B\n",
      "blocks.5.mlp.fc1.A\n",
      "blocks.5.mlp.fc1.B\n",
      "blocks.5.mlp.fc2.A\n",
      "blocks.5.mlp.fc2.B\n",
      "blocks.6.attn.qkv.A\n",
      "blocks.6.attn.qkv.B\n",
      "blocks.6.attn.proj.A\n",
      "blocks.6.attn.proj.B\n",
      "blocks.6.mlp.fc1.A\n",
      "blocks.6.mlp.fc1.B\n",
      "blocks.6.mlp.fc2.A\n",
      "blocks.6.mlp.fc2.B\n",
      "blocks.7.attn.qkv.A\n",
      "blocks.7.attn.qkv.B\n",
      "blocks.7.attn.proj.A\n",
      "blocks.7.attn.proj.B\n",
      "blocks.7.mlp.fc1.A\n",
      "blocks.7.mlp.fc1.B\n",
      "blocks.7.mlp.fc2.A\n",
      "blocks.7.mlp.fc2.B\n",
      "blocks.8.attn.qkv.A\n",
      "blocks.8.attn.qkv.B\n",
      "blocks.8.attn.proj.A\n",
      "blocks.8.attn.proj.B\n",
      "blocks.8.mlp.fc1.A\n",
      "blocks.8.mlp.fc1.B\n",
      "blocks.8.mlp.fc2.A\n",
      "blocks.8.mlp.fc2.B\n",
      "blocks.9.attn.qkv.A\n",
      "blocks.9.attn.qkv.B\n",
      "blocks.9.attn.proj.A\n",
      "blocks.9.attn.proj.B\n",
      "blocks.9.mlp.fc1.A\n",
      "blocks.9.mlp.fc1.B\n",
      "blocks.9.mlp.fc2.A\n",
      "blocks.9.mlp.fc2.B\n",
      "blocks.10.attn.qkv.A\n",
      "blocks.10.attn.qkv.B\n",
      "blocks.10.attn.proj.A\n",
      "blocks.10.attn.proj.B\n",
      "blocks.10.mlp.fc1.A\n",
      "blocks.10.mlp.fc1.B\n",
      "blocks.10.mlp.fc2.A\n",
      "blocks.10.mlp.fc2.B\n",
      "blocks.11.attn.qkv.A\n",
      "blocks.11.attn.qkv.B\n",
      "blocks.11.attn.proj.A\n",
      "blocks.11.attn.proj.B\n",
      "blocks.11.mlp.fc1.A\n",
      "blocks.11.mlp.fc1.B\n",
      "blocks.11.mlp.fc2.A\n",
      "blocks.11.mlp.fc2.B\n",
      "blocks.12.attn.qkv.A\n",
      "blocks.12.attn.qkv.B\n",
      "blocks.12.attn.proj.A\n",
      "blocks.12.attn.proj.B\n",
      "blocks.12.mlp.fc1.A\n",
      "blocks.12.mlp.fc1.B\n",
      "blocks.12.mlp.fc2.A\n",
      "blocks.12.mlp.fc2.B\n",
      "blocks.13.attn.qkv.A\n",
      "blocks.13.attn.qkv.B\n",
      "blocks.13.attn.proj.A\n",
      "blocks.13.attn.proj.B\n",
      "blocks.13.mlp.fc1.A\n",
      "blocks.13.mlp.fc1.B\n",
      "blocks.13.mlp.fc2.A\n",
      "blocks.13.mlp.fc2.B\n",
      "blocks.14.attn.qkv.A\n",
      "blocks.14.attn.qkv.B\n",
      "blocks.14.attn.proj.A\n",
      "blocks.14.attn.proj.B\n",
      "blocks.14.mlp.fc1.A\n",
      "blocks.14.mlp.fc1.B\n",
      "blocks.14.mlp.fc2.A\n",
      "blocks.14.mlp.fc2.B\n",
      "blocks.15.attn.qkv.A\n",
      "blocks.15.attn.qkv.B\n",
      "blocks.15.attn.proj.A\n",
      "blocks.15.attn.proj.B\n",
      "blocks.15.mlp.fc1.A\n",
      "blocks.15.mlp.fc1.B\n",
      "blocks.15.mlp.fc2.A\n",
      "blocks.15.mlp.fc2.B\n",
      "blocks.16.attn.qkv.A\n",
      "blocks.16.attn.qkv.B\n",
      "blocks.16.attn.proj.A\n",
      "blocks.16.attn.proj.B\n",
      "blocks.16.mlp.fc1.A\n",
      "blocks.16.mlp.fc1.B\n",
      "blocks.16.mlp.fc2.A\n",
      "blocks.16.mlp.fc2.B\n",
      "blocks.17.attn.qkv.A\n",
      "blocks.17.attn.qkv.B\n",
      "blocks.17.attn.proj.A\n",
      "blocks.17.attn.proj.B\n",
      "blocks.17.mlp.fc1.A\n",
      "blocks.17.mlp.fc1.B\n",
      "blocks.17.mlp.fc2.A\n",
      "blocks.17.mlp.fc2.B\n",
      "blocks.18.attn.qkv.A\n",
      "blocks.18.attn.qkv.B\n",
      "blocks.18.attn.proj.A\n",
      "blocks.18.attn.proj.B\n",
      "blocks.18.mlp.fc1.A\n",
      "blocks.18.mlp.fc1.B\n",
      "blocks.18.mlp.fc2.A\n",
      "blocks.18.mlp.fc2.B\n",
      "blocks.19.attn.qkv.A\n",
      "blocks.19.attn.qkv.B\n",
      "blocks.19.attn.proj.A\n",
      "blocks.19.attn.proj.B\n",
      "blocks.19.mlp.fc1.A\n",
      "blocks.19.mlp.fc1.B\n",
      "blocks.19.mlp.fc2.A\n",
      "blocks.19.mlp.fc2.B\n",
      "blocks.20.attn.qkv.A\n",
      "blocks.20.attn.qkv.B\n",
      "blocks.20.attn.proj.A\n",
      "blocks.20.attn.proj.B\n",
      "blocks.20.mlp.fc1.A\n",
      "blocks.20.mlp.fc1.B\n",
      "blocks.20.mlp.fc2.A\n",
      "blocks.20.mlp.fc2.B\n",
      "blocks.21.attn.qkv.A\n",
      "blocks.21.attn.qkv.B\n",
      "blocks.21.attn.proj.A\n",
      "blocks.21.attn.proj.B\n",
      "blocks.21.mlp.fc1.A\n",
      "blocks.21.mlp.fc1.B\n",
      "blocks.21.mlp.fc2.A\n",
      "blocks.21.mlp.fc2.B\n",
      "blocks.22.attn.qkv.A\n",
      "blocks.22.attn.qkv.B\n",
      "blocks.22.attn.proj.A\n",
      "blocks.22.attn.proj.B\n",
      "blocks.22.mlp.fc1.A\n",
      "blocks.22.mlp.fc1.B\n",
      "blocks.22.mlp.fc2.A\n",
      "blocks.22.mlp.fc2.B\n",
      "blocks.23.attn.qkv.A\n",
      "blocks.23.attn.qkv.B\n",
      "blocks.23.attn.proj.A\n",
      "blocks.23.attn.proj.B\n",
      "blocks.23.mlp.fc1.A\n",
      "blocks.23.mlp.fc1.B\n",
      "blocks.23.mlp.fc2.A\n",
      "blocks.23.mlp.fc2.B\n",
      "blocks.24.attn.qkv.A\n",
      "blocks.24.attn.qkv.B\n",
      "blocks.24.attn.proj.A\n",
      "blocks.24.attn.proj.B\n",
      "blocks.24.mlp.fc1.A\n",
      "blocks.24.mlp.fc1.B\n",
      "blocks.24.mlp.fc2.A\n",
      "blocks.24.mlp.fc2.B\n",
      "blocks.25.attn.qkv.A\n",
      "blocks.25.attn.qkv.B\n",
      "blocks.25.attn.proj.A\n",
      "blocks.25.attn.proj.B\n",
      "blocks.25.mlp.fc1.A\n",
      "blocks.25.mlp.fc1.B\n",
      "blocks.25.mlp.fc2.A\n",
      "blocks.25.mlp.fc2.B\n",
      "blocks.26.attn.qkv.A\n",
      "blocks.26.attn.qkv.B\n",
      "blocks.26.attn.proj.A\n",
      "blocks.26.attn.proj.B\n",
      "blocks.26.mlp.fc1.A\n",
      "blocks.26.mlp.fc1.B\n",
      "blocks.26.mlp.fc2.A\n",
      "blocks.26.mlp.fc2.B\n",
      "blocks.27.attn.qkv.A\n",
      "blocks.27.attn.qkv.B\n",
      "blocks.27.attn.proj.A\n",
      "blocks.27.attn.proj.B\n",
      "blocks.27.mlp.fc1.A\n",
      "blocks.27.mlp.fc1.B\n",
      "blocks.27.mlp.fc2.A\n",
      "blocks.27.mlp.fc2.B\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "  if param.requires_grad:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c79fb9-85cf-43b9-ace3-bcd045cf7071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52a6256f-1127-4622-8d9e-4e300fbdb0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8082e3e7244545b8c871332e216b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8555b2cedaea4ddeabce73f7b830e445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 15.88 MiB is free. Including non-PyTorch memory, this process has 19.44 GiB memory in use. Of the allocated memory 18.72 GiB is allocated by PyTorch, and 514.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m x\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# print(x.requires_grad_())\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/DiT/diffusion/respace.py:97\u001b[0m, in \u001b[0;36mSpacedDiffusion.training_losses\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_losses\u001b[39m(\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     96\u001b[0m ):  \u001b[38;5;66;03m# pylint: disable=signature-differs\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DiT/diffusion/gaussian_diffusion.py:747\u001b[0m, in \u001b[0;36mGaussianDiffusion.training_losses\u001b[0;34m(self, model, x_start, t, model_kwargs, noise)\u001b[0m\n\u001b[1;32m    745\u001b[0m         terms[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_type \u001b[38;5;241m==\u001b[39m LossType\u001b[38;5;241m.\u001b[39mMSE \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_type \u001b[38;5;241m==\u001b[39m LossType\u001b[38;5;241m.\u001b[39mRESCALED_MSE:\n\u001b[0;32m--> 747\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_var_type \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    750\u001b[0m         ModelVarType\u001b[38;5;241m.\u001b[39mLEARNED,\n\u001b[1;32m    751\u001b[0m         ModelVarType\u001b[38;5;241m.\u001b[39mLEARNED_RANGE,\n\u001b[1;32m    752\u001b[0m     ]:\n\u001b[1;32m    753\u001b[0m         B, C \u001b[38;5;241m=\u001b[39m x_t\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/DiT/diffusion/respace.py:129\u001b[0m, in \u001b[0;36m_WrappedModel.__call__\u001b[0;34m(self, x, ts, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m new_ts \u001b[38;5;241m=\u001b[39m map_tensor[ts]\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# if self.rescale_timesteps:\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m#     new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_ts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/qdit/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/qdit/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/DiT/models.py:245\u001b[0m, in \u001b[0;36mDiT.forward\u001b[0;34m(self, x, t, y)\u001b[0m\n\u001b[1;32m    243\u001b[0m c \u001b[38;5;241m=\u001b[39m t \u001b[38;5;241m+\u001b[39m y                                \u001b[38;5;66;03m# (N, D)\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 245\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m                      \u001b[38;5;66;03m# (N, T, D)\u001b[39;00m\n\u001b[1;32m    246\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(x, c)                \u001b[38;5;66;03m# (N, T, patch_size ** 2 * out_channels)\u001b[39;00m\n\u001b[1;32m    247\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpatchify(x)                   \u001b[38;5;66;03m# (N, out_channels, H, W)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/qdit/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/qdit/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/DiT/models.py:120\u001b[0m, in \u001b[0;36mDiTBlock.forward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, c):\n\u001b[1;32m    119\u001b[0m     shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madaLN_modulation(c)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m6\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m gate_msa\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodulate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_msa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_msa\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m gate_mlp\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(modulate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x), shift_mlp, scale_mlp))\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/qdit/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/qdit/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/qdit/lib/python3.9/site-packages/timm/models/vision_transformer.py:87\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     86\u001b[0m     B, N, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 87\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqkv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(B, N, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     88\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     89\u001b[0m     q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_norm(q), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_norm(k)\n",
      "File \u001b[0;32m~/.conda/envs/qdit/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/qdit/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m, in \u001b[0;36mLoRALayer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     20\u001b[0m     lora_adjustment \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m# (batch_size, in_features) -> (batch_size, out_features)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_layer(x) \u001b[38;5;241m+\u001b[39m \u001b[43mlora_adjustment\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 108.00 MiB. GPU 0 has a total capacity of 19.50 GiB of which 15.88 MiB is free. Including non-PyTorch memory, this process has 19.44 GiB memory in use. Of the allocated memory 18.72 GiB is allocated by PyTorch, and 514.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Training loop\n",
    "# model.train()\n",
    "# requires_grad(model, True)\n",
    "diffusion = create_diffusion(str(num_sampling_steps))\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "# Optimizer\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.1)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    running_loss = 0.0\n",
    "    for x, y in tqdm(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Encode images to latent space and normalize latents\n",
    "        with torch.no_grad():\n",
    "          x = vae.encode(x).latent_dist.sample().mul_(0.18215)\n",
    "\n",
    "        # Explicitly set requires_grad for the input latents\n",
    "        # print(x.requires_grad_())\n",
    "        # x.requires_grad_(True)\n",
    "\n",
    "        # Sample a random timestep for each batch\n",
    "        t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=device)\n",
    "        # y.requires_grad_(True)\n",
    "        model_kwargs = {\"y\": y}\n",
    "\n",
    "        # Compute training losses from diffusion\n",
    "        # t.requires_grad_(True)\n",
    "        x = torch.Tensor(x)\n",
    "        x.requires_grad_(True)\n",
    "        # print(x.requires_grad_())\n",
    "        loss_dict = diffusion.training_losses(model, x, t, model_kwargs)\n",
    "        loss = loss_dict[\"loss\"].mean()\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4839de-6d68-43a8-a5cf-091597020530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diffusion object:\n",
    "diffusion = create_diffusion(str(num_sampling_steps))\n",
    "\n",
    "# Create sampling noise:\n",
    "n = len(class_labels)\n",
    "z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
    "y = torch.tensor(class_labels, device=device)\n",
    "\n",
    "# Setup classifier-free guidance:\n",
    "z = torch.cat([z, z], 0)\n",
    "y_null = torch.tensor([1000] * n, device=device)\n",
    "y = torch.cat([y, y_null], 0)\n",
    "model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "\n",
    "# Sample images:\n",
    "samples = diffusion.p_sample_loop(\n",
    "    model.forward_with_cfg, z.shape, z, clip_denoised=False,\n",
    "    model_kwargs=model_kwargs, progress=True, device=device\n",
    ")\n",
    "samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "samples = vae.decode(samples / 0.18215).sample\n",
    "\n",
    "# Save and display images:\n",
    "save_image(samples, \"sample.png\", nrow=int(samples_per_row),\n",
    "           normalize=True, value_range=(-1, 1))\n",
    "samples = Image.open(\"sample.png\")\n",
    "display(samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
