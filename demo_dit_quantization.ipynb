{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf97bfc-a412-4c6a-86fd-4a19d32801f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from models import DiT_models\n",
    "from download import find_model\n",
    "from diffusion import create_diffusion\n",
    "from diffusers.models import AutoencoderKL\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "import argparse\n",
    "from torchvision.utils import save_image\n",
    "from timm.models.vision_transformer import Attention, Mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d2a30-1b68-4066-9de5-604ebec4b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b1f419-6867-4457-9526-ca8f565c1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144a2c4-06d1-4288-9e01-ff1f67d7851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_channel_absmax(w, n_bits=8):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_weight_per_tensor_absmax(w, n_bits=8):\n",
    "    # w: (out_features, in_features)\n",
    "    scales = w.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    w.div_(scales).round_().mul_(scales)\n",
    "    return w\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_token_absmax(t, n_bits=8):\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max(dim=-1, keepdim=True)[0]\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_activation_per_tensor_absmax(t, n_bits=8):\n",
    "    t_shape = t.shape\n",
    "    t.view(-1, t_shape[-1])\n",
    "    scales = t.abs().max()\n",
    "    q_max = 2 ** (n_bits - 1) - 1\n",
    "    scales.clamp_(min=1e-5).div_(q_max)\n",
    "    t.div_(scales).round_().mul_(scales)\n",
    "    return t\n",
    "\n",
    "\n",
    "class W8A8Linear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        bias=True,\n",
    "        act_quant=\"per_token\",\n",
    "        quantize_output=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"weight\",\n",
    "            torch.randn(\n",
    "                self.out_features,\n",
    "                self.in_features,\n",
    "                dtype=torch.float16,\n",
    "                requires_grad=False,\n",
    "            ),\n",
    "        )\n",
    "        if bias:\n",
    "            self.register_buffer(\n",
    "                \"bias\",\n",
    "                torch.zeros(\n",
    "                    (1, self.out_features), dtype=torch.float16, requires_grad=False\n",
    "                ),\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\"bias\", None)\n",
    "\n",
    "        if act_quant == \"per_token\":\n",
    "            self.act_quant_name = \"per_token\"\n",
    "            self.act_quant = partial(quantize_activation_per_token_absmax, n_bits=8)\n",
    "        elif act_quant == \"per_tensor\":\n",
    "            self.act_quant_name = \"per_tensor\"\n",
    "            self.act_quant = partial(quantize_activation_per_tensor_absmax, n_bits=8)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid act_quant: {act_quant}\")\n",
    "\n",
    "        if quantize_output:\n",
    "            self.output_quant_name = self.act_quant_name\n",
    "            self.output_quant = self.act_quant\n",
    "        else:\n",
    "            self.output_quant_name = \"None\"\n",
    "            self.output_quant = lambda x: x\n",
    "\n",
    "    def to(self, *args, **kwargs):\n",
    "        super(W8A8Linear, self).to(*args, **kwargs)\n",
    "        self.weight = self.weight.to(*args, **kwargs)\n",
    "        if self.bias is not None:\n",
    "            self.bias = self.bias.to(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x):\n",
    "        q_x = self.act_quant(x)\n",
    "        y = torch.functional.F.linear(q_x, self.weight, self.bias)\n",
    "        q_y = self.output_quant(y)\n",
    "        return q_y\n",
    "\n",
    "    @staticmethod\n",
    "    def from_float(\n",
    "        module, weight_quant=\"per_channel\", act_quant=\"per_token\", quantize_output=False\n",
    "    ):\n",
    "        assert isinstance(module, torch.nn.Linear)\n",
    "        new_module = W8A8Linear(\n",
    "            module.in_features,\n",
    "            module.out_features,\n",
    "            module.bias is not None,\n",
    "            act_quant=act_quant,\n",
    "            quantize_output=quantize_output,\n",
    "        )\n",
    "        if weight_quant == \"per_channel\":\n",
    "            new_module.weight = quantize_weight_per_channel_absmax(\n",
    "                module.weight, n_bits=4\n",
    "            )  # use 8-bit integer for weight\n",
    "        elif weight_quant == \"per_tensor\":\n",
    "            new_module.weight = quantize_weight_per_tensor_absmax(\n",
    "                module.weight, n_bits=4\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid weight_quant: {weight_quant}\")\n",
    "        new_module.weight_quant_name = weight_quant\n",
    "        if module.bias is not None:\n",
    "            new_module.bias = module.bias\n",
    "        return new_module\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"W8A8Linear({self.in_features}, {self.out_features}, bias={self.bias is not None}, weight_quant={self.weight_quant_name}, act_quant={self.act_quant_name}, output_quant={self.output_quant_name})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a91340b-8b78-4791-bc49-130f1bead010",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256 #@param [256, 512]\n",
    "vae_model = \"stabilityai/sd-vae-ft-ema\" #@param [\"stabilityai/sd-vae-ft-mse\", \"stabilityai/sd-vae-ft-ema\"]\n",
    "# Load model:\n",
    "latent_size = int(image_size) // 8\n",
    "\n",
    "def init_models():\n",
    "    model = DiT_models['DiT-XL/2'](input_size=latent_size).to(device)\n",
    "    # state_dict = find_model(f\"DiT-XL-2-{image_size}x{image_size}.pt\")\n",
    "    state_dict = torch.load('/n/netscratch/nali_lab_seas/Everyone/mingze/models/pretrained_models/DiT-XL-2-256x256.pt', weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval() # important!\n",
    "    vae = AutoencoderKL.from_pretrained(vae_model).to(device)\n",
    "\n",
    "    return model, vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9303c18e-3785-43b6-a187-b54d873f33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_dit_only_block(\n",
    "    model, weight_quant=\"per_tensor\", act_quant=\"per_tensor\", quantize_bmm_input=True\n",
    "):\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, Attention):  # quantize each linear layer within attention block\n",
    "            m.qkv = W8A8Linear.from_float(\n",
    "                m.qkv, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input\n",
    "            )\n",
    "            m.proj = W8A8Linear.from_float(\n",
    "                m.proj, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input\n",
    "            )\n",
    "        if isinstance(m, Mlp):\n",
    "            m.fc1 = W8A8Linear.from_float(\n",
    "                m.fc1, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input\n",
    "            )\n",
    "            m.fc2 = W8A8Linear.from_float(\n",
    "                m.fc2, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input\n",
    "            )\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60015d78-2585-4ebb-be50-7e90f85dd10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_dit_all_linear_layer(\n",
    "    model, weight_quant=\"per_tensor\", act_quant=\"per_tensor\", quantize_bmm_input=False\n",
    "):\n",
    "    for name, m in model.named_modules():\n",
    "        if isinstance(m, torch.nn.Linear):  # quantize each linear layer\n",
    "            m = W8A8Linear.from_float(\n",
    "                m, weight_quant=weight_quant, act_quant=act_quant, quantize_output=quantize_bmm_input\n",
    "            )\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b626a4a-d54f-4bac-bf85-2c6a99270cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_func(quant_func=lambda x: x):\n",
    "\n",
    "    # Set user inputs:\n",
    "    seed = 1 #@param {type:\"number\"}\n",
    "    torch.manual_seed(seed)\n",
    "    num_sampling_steps = 200 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
    "    cfg_scale = 2 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "    class_labels = 23, 123, 324, 405 #@param {type:\"raw\"}\n",
    "    samples_per_row = 4 #@param {type:\"number\"}\n",
    "    \n",
    "    model, vae = init_models()\n",
    "\n",
    "    model = quant_func(model)\n",
    "    \n",
    "    # Create diffusion object:\n",
    "    diffusion = create_diffusion(str(num_sampling_steps))\n",
    "    \n",
    "    # Create sampling noise:\n",
    "    n = len(class_labels)\n",
    "    z = torch.randn(n, 4, latent_size, latent_size, device=device)\n",
    "    y = torch.tensor(class_labels, device=device)\n",
    "    \n",
    "    # Setup classifier-free guidance:\n",
    "    z = torch.cat([z, z], 0)\n",
    "    y_null = torch.tensor([1000] * n, device=device)\n",
    "    y = torch.cat([y, y_null], 0)\n",
    "    model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "    \n",
    "    # Sample images:\n",
    "    samples = diffusion.p_sample_loop(\n",
    "        model.forward_with_cfg, z.shape, z, clip_denoised=False,\n",
    "        model_kwargs=model_kwargs, progress=True, device=device\n",
    "    )\n",
    "    samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "    samples = vae.decode(samples / 0.18215).sample\n",
    "    \n",
    "    # Save and display images:\n",
    "    save_image(samples, \"sample.png\", nrow=int(samples_per_row),\n",
    "               normalize=True, value_range=(-1, 1))\n",
    "    samples = Image.open(\"sample.png\")\n",
    "    display(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871f688-b63e-451a-a223-426d1b1fdc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c212ee-f77a-49b2-ac7e-964e0630c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_func(quantize_dit_all_linear_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436b1532-8aaf-4ead-b61f-6dd5f92d0166",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31442ce5-a2f7-4169-a83e-370e9d5153f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 34745\n",
       "    Root location: /n/home11/mingzeyuan/.cache/kagglehub/datasets/ifigotin/imagenetmini-1000/versions/1/imagenet-mini/train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
       "               RandomHorizontalFlip(p=0.5)\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
       "           )"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "# Dataset and Dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "# Specify the target classes\n",
    "selected_class_ids = [0, 1, 2, 3]\n",
    "\n",
    "full_dataset = ImageFolder(\"/n/home11/mingzeyuan/.cache/kagglehub/datasets/ifigotin/imagenetmini-1000/versions/1/imagenet-mini/train\", transform=transform)\n",
    "# Get indices of images belonging to the selected classes\n",
    "# selected_indices = [i for i, (_, label) in enumerate(full_dataset) if label in selected_class_ids]\n",
    "\n",
    "# Create a subset dataset with only the selected classes\n",
    "# filtered_dataset = Subset(full_dataset, selected_indices)\n",
    "# dataloader = DataLoader(filtered_dataset, batch_size=32, shuffle=False)\n",
    "full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78737b2a-0431-4ed1-8a7b-3949e9b38d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "296ec0c6-6357-4fc0-b069-e3df954b1172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fcc529bc5c42419d769207b3a52ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "selected_indices = []\n",
    "for i, (_, label) in tqdm(enumerate(full_dataset)):\n",
    "    if label in selected_class_ids:\n",
    "        selected_indices.append(i)\n",
    "    if label > max(selected_class_ids):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02d99b2b-f332-4cc4-9d57-070df87dca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = Subset(full_dataset, selected_indices)\n",
    "dataloader = DataLoader(filtered_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253393a8-6e51-414d-8a92-10fcfd5b16d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98582a-8cc0-4fa9-b037-2d45b4cebeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define LoRA Layer\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, original_layer, r=8, alpha=1.0):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.original_layer = original_layer\n",
    "        self.r = r  # Low rank\n",
    "        self.alpha = alpha  # Scaling factor\n",
    "\n",
    "        # Low-rank matrices\n",
    "        self.A = nn.Parameter(torch.randn(original_layer.out_features, r) * 0.01)\n",
    "        self.B = nn.Parameter(torch.randn(r, original_layer.in_features) * 0.01)\n",
    "\n",
    "        # Scaling factor to ensure initial LoRA impact is small\n",
    "        self.scale = alpha / self.r\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora_adjustment = (x @ self.B.T) @ self.A.T  # (batch_size, in_features) -> (batch_size, out_features)\n",
    "        return self.original_layer(x) + lora_adjustment * self.scale\n",
    "\n",
    "def add_lora_to_model(model, r=8, alpha=1.0):\n",
    "    layers_to_modify = []  # Collect layers to modify first\n",
    "\n",
    "    # Collect all linear layers in a list\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear) and (\"mlp\" in name or \"attn\" in name):\n",
    "            layers_to_modify.append((name, module))\n",
    "\n",
    "    # Replace each collected layer with a LoRA layer\n",
    "    for name, module in layers_to_modify:\n",
    "        # Split the name by '.' to traverse submodules and set the new layer correctly\n",
    "        submodule = model\n",
    "        *module_names, layer_name = name.split(\".\")\n",
    "        for module_name in module_names:\n",
    "            submodule = getattr(submodule, module_name)\n",
    "\n",
    "        # Replace the layer with a LoRA layer\n",
    "        setattr(submodule, layer_name, LoRALayer(module, r=r, alpha=alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15464d3a-ac46-4d0b-9d6f-11bb6526f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `model` is the DiT model with LoRA layers added\n",
    "def freeze_model_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if not ((\".A\" in name) or ('.B' in name)):  # Replace with the identifier for LoRA parameters\n",
    "            param.requires_grad = False  # Freeze base model weights\n",
    "\n",
    "# Add LoRA layers (as shown in previous responses) and freeze base model weights\n",
    "# add_lora_to_model(model)  # Add LoRA layers to the model\n",
    "# freeze_model_weights(model)  # Freeze original model weights\n",
    "# model.to(\"cuda\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6901a-e8f5-417c-a4d0-bf7bbdb90ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Training loop\n",
    "# model.train()\n",
    "# requires_grad(model, True)\n",
    "epochs = 500\n",
    "\n",
    "# Optimizer\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.1)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    running_loss = 0.0\n",
    "    for x, y in tqdm(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Encode images to latent space and normalize latents\n",
    "        with torch.no_grad():\n",
    "          x = vae.encode(x).latent_dist.sample().mul_(0.18215)\n",
    "\n",
    "        # Explicitly set requires_grad for the input latents\n",
    "        # print(x.requires_grad_())\n",
    "        # x.requires_grad_(True)\n",
    "\n",
    "        # Sample a random timestep for each batch\n",
    "        t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=device)\n",
    "        # y.requires_grad_(True)\n",
    "        model_kwargs = {\"y\": y}\n",
    "\n",
    "        # Compute training losses from diffusion\n",
    "        # t.requires_grad_(True)\n",
    "        x = torch.Tensor(x)\n",
    "        x.requires_grad_(True)\n",
    "        # print(x.requires_grad_())\n",
    "        loss_dict = diffusion.training_losses(model, x, t, model_kwargs)\n",
    "        loss = loss_dict[\"loss\"].mean()\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print epoch loss\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
